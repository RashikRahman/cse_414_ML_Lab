import pandas as pd
import numpy as np


# parse_dates=["day"] converts the "day" column into pandas datetime objects.
# This is important for time-series operations like time interpolation.
df = pd.read_csv("weather_data.csv", parse_dates=["day"])


print(df.dtypes)


df = df.set_index("day")
df.head()


df.reset_index(inplace=True)
df


df.isna().sum()





df_fill_all_zero = df.fillna(0)
df_fill_all_zero.head()


df_fill_dict = df.fillna({
    "temperature": 0,       # numeric fill: sometimes 0, mean, median, etc.
    "windspeed": 1,         # numeric fill
    "event": "No Event"     # categorical fill
})

df_fill_dict.head()


# Forward fill (ffill): propagate last valid observation forward
# Good for time series when "last known value still holds".
df_ffill = df.fillna(method="ffill")
df_ffill


# Backward fill (bfill): use next valid observation to fill previous NaNs
# Sometimes used when the next known label/value is assumed to apply earlier.
df_bfill = df.fillna(method="bfill")
df_bfill


# limit: only fill up to N consecutive NaNs
# Useful to avoid filling long gaps with stale values.
df_ffill_limit1 = df.fillna(method="ffill", limit=1)

df_ffill_limit1





#Time interpolation uses the DatetimeIndex
# It accounts for actual time gaps between observations.
# Only works properly if your index is datetime.

df_day = df.set_index('day')
df_interp_time = df_day.interpolate(method="time")

df_interp_time


#drop rows that contain ANY missing value
# This can shrink your dataset quickly (bad if many NaNs).
df_drop_any = df.dropna()

print(df_drop_any)
print("Remaining rows:", len(df_drop_any))


#drop rows only if ALL values are missing in that row
# This is less aggressive.
df_drop_all = df.dropna(how="all")

print(df_drop_all.head())
print("Remaining rows:", len(df_drop_all))


#thresh: keep rows with at least K non-NaN values
# Example: thresh=2 means at least 2 columns must be non-missing.
df_drop_thresh2 = df.dropna(thresh=2)

print(df_drop_thresh2)
print("Remaining rows:", len(df_drop_thresh2))





df = pd.read_csv("weather_data2.csv")

print(df)
print(df.dtypes)



# Many datasets use -99999 to mean "missing" (but pandas won't treat it as missing automatically).
# Replace that sentinel with actual NaN so pandas missing-data tools can work.
df_1 = df.replace(-99999, np.nan)

df_1


# Sometimes a dataset uses multiple sentinel codes: -99999, -88888, etc.
# Here we replace both with 0 (or could be NaN depending on meaning).


# - Replacing to 0 is only OK if 0 is a meaningful real value.
# - If it means "unknown", replace with np.nan and later impute properly.

df_2 = df.replace(to_replace=[-99999, -88888], value=0)

df_2


# Sometimes sentinel values differ by column OR you only want to clean specific columns.
# This syntax says:
#   - in 'temperature' column replace -99999 with NaN
#   - in 'windspeed'   column replace -99999 with NaN
#   - in 'event'       column replace string '0' with NaN (meaning "no event recorded")

df_3 = df.replace({
    "temperature": -99999,
    "windspeed": -99999,
    "event": "0"
}, np.nan)


df_3


# This style is global: anywhere in the dataframe,
# replace -99999 -> NaN and "no event" -> "Sunny"
# (In the original notebook they show mapping style usage.)

# Be careful with global mapping: if "-99999" appears in a column where it is a valid value,
# it will still be replaced. Per-column mapping is safer.

df_4 = df.replace({
    -99999: np.nan,
    "no event": "Sunny"
})


print(df_4)


# Regex replacement: cleaning units like 'F' and 'mph'

# In the dataset, temperature and windspeed may include units in strings like:
# "32 F", "28 c", "6 mph", "7 mph"
#
# We can remove alphabet characters using regex.
# Example pattern: '[A-Za-z]' means "any letter (A-Z or a-z)".
#

df_5 = df.replace(
    {"temperature": r"[A-Za-z]", "windspeed": r"[A-Za-z]"},
    "",
    regex=True
)


print(df_5)

# N.B:
# After removing letters, the columns may still be strings like "32  " or " 6  ".
# We should strip spaces and convert to numeric for ML models.

df_5["temperature"] = pd.to_numeric(df_5["temperature"].astype(str).str.strip(), errors="coerce")
df_5["windspeed"]   = pd.to_numeric(df_5["windspeed"].astype(str).str.strip(), errors="coerce")


print(df_5)
print(df_5.dtypes)

# N.B:
# errors="coerce" means: anything still not parseable becomes NaN.
# This is useful when cleaning messy real-world data.



# Replace a list with another list (category -> numeric score)

# This is a common ML step: convert ordinal categories into ordered numbers.
# Example: score labels -> numeric ratings.

grades = pd.DataFrame({
    "score": ["exceptional", "average", "good", "poor", "average", "exceptional"],
    "student": ["rob", "maya", "parthiv", "tom", "julian", "erica"]
})

print("\n=== Grades dataframe ===")
print(grades)

# Replace multiple categories using list-to-list mapping.
# RULE: the two lists must have the SAME length.

grades_num = grades.replace(
    ["poor", "average", "good", "exceptional"],
    [1, 2, 3, 4]
)


print(grades_num)

# ML note:
# This makes sense ONLY if the categories are truly ordinal (poor < average < good < exceptional).
# If categories are nominal (e.g., city names), use one-hot encoding instead.












